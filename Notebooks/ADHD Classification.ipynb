{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-685689bdd009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TF_KERAS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "import onnx\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import TimeDistributed\n",
    "from nilearn.masking import apply_mask, unmask\n",
    "from nilearn.image import resample_to_img\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import compute_epi_mask\n",
    "from nibabel.testing import data_path\n",
    "import nibabel as nib\n",
    "from keras.layers import LSTM\n",
    "import nilearn\n",
    "import argparse\n",
    "import keras\n",
    "import keras.backend\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.utils\n",
    "#import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense\n",
    "#from tensorflow.keras.layers import Conv2D,Dropout,Flatten,Activation,MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "#from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import nipype.interfaces.io as nio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "from tqdm import tqdm\n",
    "import scipy.io as sio\n",
    "from nilearn.image import index_img\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from nilearn.input_data import NiftiLabelsMasker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting labels \n",
    "phenotypic = pd.read_csv('/Users/pranjal27bhardwaj/Desktop/allSubs_testSet_phenotypic_dx.csv')\n",
    "labels = phenotypic['DX']\n",
    "labels = pd.DataFrame(labels)\n",
    "ID = phenotypic['ScanDir ID']\n",
    "ID = pd.DataFrame(ID)\n",
    "result = pd.concat([ID,labels], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting fmri image path of one website\n",
    "nifti_files = glob.glob('/Users/desktop/fmri_KKI/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating Id from the list of path \n",
    "s_nifti = []\n",
    "for i in nifti_files:\n",
    "    s_nifti.append(re.split('(\\d+)',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separting ID from the list of path \n",
    "s1_nifti = []\n",
    "for i in range(len(s_nifti)):\n",
    "    s1_nifti.append(s_nifti[i][1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#loading one subject of FMRI file\n",
    "#converting images into numpy array\n",
    "arr = []\n",
    "for i in range(len(nifti_files)):\n",
    "    Img = nib.load(nifti_files[i])\n",
    "    x = Img.get_data()\n",
    "    arr.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the voxels\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "\n",
    "# Load data\n",
    "nii = nb.load(nifti_files[0])\n",
    "img = nii.get_fdata()\n",
    "\n",
    "# Get voxel dimensions\n",
    "voxel_dims = (nii.header[\"pixdim\"])[1:4]\n",
    "print(\"Voxel dimensions:\")\n",
    "print(\"  x = {} mm\".format(voxel_dims[0]))\n",
    "print(\"  y = {} mm\".format(voxel_dims[1]))\n",
    "print(\"  z = {} mm\".format(voxel_dims[2]))\n",
    "\n",
    "# Compute volume\n",
    "nonzero_voxel_count = np.count_nonzero(img)\n",
    "voxel_volume = np.prod(voxel_dims)\n",
    "nonzero_voxel_volume = nonzero_voxel_count * voxel_volume\n",
    "\n",
    "print(\"Number of non-zero voxels = {}\".format(nonzero_voxel_count))\n",
    "print(\"Volume of non-zero voxels = {} mm^3\".format(nonzero_voxel_volume))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_img = index_img(nifti_files[0], slice(0, 100))\n",
    "masker = NiftiMasker(mask_strategy='epi')\n",
    "s1 = masker.fit(epi_img)\n",
    "type(s1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We first create a masker, giving it the options that we care\n",
    "# about. Here we use standardizing of the data, as it is often important\n",
    "# for decoding\n",
    "\n",
    "masker = NiftiMasker(standardize = True)\n",
    "fmri_masked = masker.fit_transform(nifti_files[0])\n",
    "coef_img = masker.inverse_transform(fmri_masked)\n",
    "plotting.plot_stat_map(coef_img)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_masked.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Img_1 = nib.load(nifti_files[0])\n",
    "Niak_1 = image.index_img(Img_1, 1)\n",
    "print(Niak_1.shape)\n",
    "Niak_info = Niak_1.header\n",
    "#print(Niak_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorting labels according to file\n",
    "\n",
    "Id = result['ScanDir ID'].values.tolist()\n",
    "label = result['DX'].values.tolist()\n",
    "#df2 = s1_nifti[0].values.tolist() \n",
    "\n",
    "df_sort = []\n",
    "for i in range(len(s1_nifti)):\n",
    "    df_sort.append(int(s1_nifti[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting the labels according to file\n",
    "\n",
    "new_list = []\n",
    "for i in range(len(df_sort)):\n",
    "    if df_sort[i] in Id:\n",
    "        s = Id.index(df_sort[i])\n",
    "        #print(df_sort[i])\n",
    "        new_list.append(label[s])\n",
    "        #print(label[s])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Preprocessing Visualization of one instance without ADHD\n",
    "data = arr \n",
    "#mean_func = image.mean_img(Niak_)\n",
    "\n",
    "#smoothing  with a varying amount of smoothing, from none to 14mm by increment of 2mm\n",
    "for smoothing in range(0, 3, 1):\n",
    "    smoothed_img = image.smooth_img(mean_func, smoothing)\n",
    "    plotting.plot_epi(smoothed_img,title=\"Smoothing %imm\" % smoothing)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#smoothing image with ADHD instance\n",
    "mean_func = image.mean_img(adhd)\n",
    "\n",
    "#smoothing  with a varying amount of smoothing, from none to 14mm by increment of 2mm\n",
    "for smoothing in range(0, 3, 1):\n",
    "    smoothed_img = image.smooth_img(mean_func, smoothing)\n",
    "    plotting.plot_epi(smoothed_img,title=\"Smoothing %imm\" % smoothing)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Performing smoothing and cleaning of image on whole dataset and saving it to another folder\n",
    "for i in range(len(nifti_files)):\n",
    "    smoothed_img = image.smooth_img(nifti_files[i], 2)\n",
    "    s = image.clean_img(smoothed_img, sessions=None, detrend=True ,standardize= True)\n",
    "    s2 = image.index_img(s,10)\n",
    "    nib.save(s2 , '/Users/vidhi/Desktop/KKI_data/' + s1_nifti[i] +'.nii.gz')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#performing 2mm smoothing on whole data\n",
    "#reshaping the dimensions of the data before passing to  cnn\n",
    "Dataset = []\n",
    "for i in range(82):\n",
    "    #masker = NiftiMasker(standardize = True, detrend = True)\n",
    "    #Dataset.append(masker.fit_transform(nifti_files[i]))\n",
    "    Img = nib.load('/Users/vidhi/Desktop/KKI_data/' + s1_nifti[i] +'.nii.gz')\n",
    "    x = Img.get_data()\n",
    "    #result = x[:, :, :,:1]\n",
    "    Dataset.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53, 64, 46)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Img = nib.load('/Users/vidhi/Desktop/KKI_data/' + s1_nifti[2] +'.nii.gz')\n",
    "x = Img.get_data()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#performing 2mm smoothing on whole data\n",
    "#reshaping the dimensions of the data before passing to  cnn\n",
    "Dataset = []\n",
    "for i in range(82):\n",
    "    Img = nib.load('/Users/vidhi/Desktop/KKI_data/' + s1_nifti[i] +'.nii.gz')\n",
    "    x = Img.get_data()\n",
    "    #result = x[:, :, :,:1]\n",
    "    Dataset.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(Dataset)\n",
    "Y = np.array(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary classification\n",
    "for i in range(len(new_list)):\n",
    "    if new_list[i] > 1:\n",
    "        new_list[i] = 1\n",
    "    else:\n",
    "        continue \n",
    "        \n",
    "#X = np.array([Dataset], dtype = object)\n",
    "#Y = np.array(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 60, 1: 16, 2: 1, 3: 5})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 53, 64, 46)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OHSU dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting labels \n",
    "phenotypic_ohsu = pd.read_csv('/Users/vidhi/Desktop/OHSU_phenotypic.csv')\n",
    "labels_ohsu = phenotypic_ohsu['DX']\n",
    "labels_ohsu = pd.DataFrame(labels_ohsu)\n",
    "ID_ohsu = phenotypic_ohsu['ScanDir ID']\n",
    "ID_ohsu = pd.DataFrame(ID_ohsu)\n",
    "result_ohsu = pd.concat([ID_ohsu,labels_ohsu], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohsu_files = glob.glob('/Users/vidhi/Desktop/fmri_ohsu 2/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = [x for x in ohsu_files if re.search('run1', x)]\n",
    "ohsu = []\n",
    "for i in img_path:\n",
    "    ohsu.append(re.split('(\\d+)',i))\n",
    "ohsu_id = []\n",
    "for i in range(len(ohsu)):\n",
    "    ohsu_id.append(ohsu[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorting labels according to file\n",
    "\n",
    "Id_ohsu = result_ohsu['ScanDir ID'].values.tolist()\n",
    "label_ohsu = result_ohsu['DX'].values.tolist()\n",
    "#df2 = s1_nifti[0].values.tolist() \n",
    "\n",
    "id_int = []\n",
    "for i in range(len(ohsu_id)):\n",
    "    id_int.append(int(ohsu_id[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting the labels according to file\n",
    "sorted_labels = []\n",
    "for i in range(len(id_int)):\n",
    "    if id_int[i] in Id_ohsu:\n",
    "        s1 = Id_ohsu.index(id_int[i])\n",
    "        #print(id_int[i])\n",
    "        sorted_labels.append(label_ohsu[s1])\n",
    "        #print(label_ohsu[s1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Performing smoothing and cleaning of image on whole dataset and saving it to another folder\n",
    "ohsu_data = []\n",
    "for i in range(len(img_path)):\n",
    "    masker = NiftiMasker(standardize = True, detrend = True)\n",
    "    ohsu_data.append(masker.fit_transform(img_path[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Performing smoothing and cleaning of image on whole dataset and saving it to another folder\n",
    "ohsu_data = []\n",
    "for i in range(len(img_path)):\n",
    "    smoothing = image.smooth_img(img_path[i], 2)\n",
    "    new_img = image.clean_img(smoothing, sessions=None, detrend=True ,standardize= True)\n",
    "    s1 = image.index_img(new_img,10)\n",
    "    x1 = s1.get_data()\n",
    "    ohsu_data.append(x1)\n",
    "    \n",
    "    #nib.save(s2 , '/Users/vidhi/Desktop/KKI_data/' + s1_nifti[i] +'.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ohsu = np.array(ohsu_data)\n",
    "Y_ohsu = np.array(sorted_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Binary classification\n",
    "for i in range(len(sorted_labels)):\n",
    "    if sorted_labels[i] > 1:\n",
    "        sorted_labels[i] = 1\n",
    "    else:\n",
    "        continue \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ohsu = np.array(ohsu_data)\n",
    "Y_ohsu = np.array(sorted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 30, 0: 42, 3: 5, 2: 2})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the labels of \n",
    "Counter(Y_ohsu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining both dataset\n",
    "x_combined = np.concatenate((X, X_ohsu))\n",
    "y_combined = np.concatenate((Y,Y_ohsu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 102, 1: 46, 2: 3, 3: 10})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using counter to check if data is balanced or not\n",
    "Counter(y_combined)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#performing upsampling \n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(x_combined, y_combined)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#path \n",
    "path = []\n",
    "Id = []\n",
    "for i in range(len(s1_nifti)):\n",
    "    path.append('/Users/vidhi/Desktop/KKI_data/' + s1_nifti[i] +'.nii.gz')\n",
    "    Id.append(s1_nifti[i])\n",
    "    \n",
    "#Creating a dataframe of whole dataset path\n",
    "Data = pd.DataFrame (path,columns=['data_path'])\n",
    "ID = pd.DataFrame(Id,columns=['ID'])\n",
    "label = pd.DataFrame(new_list,columns=['labels'])    \n",
    "df = pd.concat([ID,Data,label], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#performing z-score normalized\n",
    "def zscore_normalize(img, mask=None):\n",
    "    \"\"\"\n",
    "    normalize a image by subtracting the mean of the whole brain\n",
    "    and dividing by the standard deviation\n",
    "    \"\"\"\n",
    "\n",
    "    img_data = img.get_data()\n",
    "    if mask is not None and not isinstance(mask, str):\n",
    "        mask_data = mask.get_data()\n",
    "    elif mask == 'nomask':\n",
    "        mask_data = img_data == img_data\n",
    "    else:\n",
    "        mask_data = img_data > img_data.mean()\n",
    "    logical_mask = mask_data == 1  # force the mask to be logical type\n",
    "    mean = img_data[logical_mask].mean()\n",
    "    std = img_data[logical_mask].std()\n",
    "    normalized = nib.Nifti1Image((img_data - mean) / std, img.affine, img.header)\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#performing gaussian mixture model normalization in all the dataset\n",
    "normalized_data = []\n",
    "for i in range(len(s1_nifti)):\n",
    "    Img = nib.load('/Users/Desktop/KKI_data/' + s1_nifti[i] +'.nii.gz')\n",
    "    #norm = zscore_normalize(Img)\n",
    "    new_img = Img.get_data()\n",
    "    normalized_data.append(new_img)\n",
    "    #nib.save(norm , '/Users/Desktop/Gaussian normalized/' + s1_nifti[i] +'.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the train data value\n",
    "reshaped_data = []\n",
    "for i in range(len(x_combined)):\n",
    "    reshaped_data.append(x_combined[i].reshape(1,53,64,46))\n",
    "X_data = np.array(reshaped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to categorical\n",
    "Y_new = keras.utils.to_categorical(y_combined, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_data,Y_new,shuffle = True, random_state = 32, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_27 (TimeDis (None, None, 53, 64, 32)  13280     \n",
      "_________________________________________________________________\n",
      "time_distributed_28 (TimeDis (None, None, 53, 64, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_29 (TimeDis (None, None, 26, 32, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, None, 26, 32, 32)  9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, None, 13, 16, 32)  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, None, 13, 16, 32)  9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, None, 6, 8, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis (None, None, 6, 8, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis (None, None, 3, 4, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_36 (TimeDis (None, None, 3, 4, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_37 (TimeDis (None, None, 3, 4, 32)    9248      \n",
      "_________________________________________________________________\n",
      "time_distributed_38 (TimeDis (None, None, 3, 4, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_39 (TimeDis (None, None, 384)         0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, None, 100)         194000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               25250     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 1004      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 422,924\n",
      "Trainable params: 422,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#model making\n",
    "model = keras.models.Sequential() \n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(32, kernel_size= 3, kernel_initializer='uniform', padding='same'),\n",
    "                                              input_shape=(None,53,64,46)))\n",
    "\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Activation(\"relu\")))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=2)))\n",
    "#model.add(keras.layers.TimeDistributed(keras.layers.Dropout(0.2)))\n",
    "\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(32, kernel_size= 3, kernel_initializer='uniform', padding='same',\n",
    "                              activation = 'relu')))\n",
    "model.add(TimeDistributed(keras.layers.MaxPool2D(pool_size=2)))\n",
    "\n",
    "\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(32, kernel_size= 3, kernel_initializer='uniform', padding='same',\n",
    "                              activation='relu')))\n",
    "model.add(TimeDistributed(keras.layers.MaxPool2D(pool_size = 2)))\n",
    "\n",
    "\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(32, kernel_size= 3, kernel_initializer='uniform', padding='same',\n",
    "                              activation='relu')))\n",
    "model.add(TimeDistributed(keras.layers.MaxPool2D(pool_size = 2)))\n",
    "\n",
    "\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(32, kernel_size= 3, kernel_initializer='uniform', padding='same',\n",
    "                              activation='relu')))\n",
    "#model.add(TimeDistributed(keras.layers.MaxPool2D(pool_size = 2)))\n",
    "\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Conv2D(32, kernel_size= 3, kernel_initializer='uniform', padding='same',\n",
    "                              activation='relu')))\n",
    "#model.add(TimeDistributed(keras.layers.MaxPool2D(pool_size=2)))\n",
    "\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dropout(0.2)))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Flatten()))\n",
    "\n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(keras.layers.Dense(250, kernel_initializer='he_normal',activation = 'relu'))\n",
    "model.add(keras.layers.Dense(250, kernel_initializer='he_normal',activation = 'relu'))\n",
    "#model.add(keras.layers.Dense(512, kernel_initializer='he_normal',activation = 'relu'))\n",
    "model.add(keras.layers.Dense(4))\n",
    "model.add(keras.layers.Activation('softmax'))\n",
    "#model.add(Activation('linear'))\n",
    "    \n",
    "#model.add(keras.layers.Activation('softmax'))\n",
    "    \n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#model.build(input_shape)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(x_train, y_train,epochs= 200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "(116, 1, 53, 64, 46)\n",
      "WARNING:tensorflow:From /Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 116 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/vidhi/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "116/116 [==============================] - 3s 24ms/step - loss: 1.3702 - acc: 0.5172 - val_loss: 1.3139 - val_acc: 0.7500\n",
      "Epoch 2/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 1.2725 - acc: 0.6121 - val_loss: 1.0047 - val_acc: 0.7500\n",
      "Epoch 3/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9946 - acc: 0.6121 - val_loss: 0.6960 - val_acc: 0.7500\n",
      "Epoch 4/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9806 - acc: 0.6121 - val_loss: 0.7104 - val_acc: 0.7500\n",
      "Epoch 5/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9412 - acc: 0.6121 - val_loss: 0.7663 - val_acc: 0.7500\n",
      "Epoch 6/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9308 - acc: 0.6121 - val_loss: 0.7841 - val_acc: 0.7500\n",
      "Epoch 7/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9347 - acc: 0.6121 - val_loss: 0.7799 - val_acc: 0.7500\n",
      "Epoch 8/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9288 - acc: 0.6121 - val_loss: 0.7554 - val_acc: 0.7500\n",
      "Epoch 9/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9268 - acc: 0.6121 - val_loss: 0.7334 - val_acc: 0.7500\n",
      "Epoch 10/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9253 - acc: 0.6121 - val_loss: 0.7287 - val_acc: 0.7500\n",
      "Epoch 11/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9229 - acc: 0.6121 - val_loss: 0.7267 - val_acc: 0.7500\n",
      "Epoch 12/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9238 - acc: 0.6121 - val_loss: 0.7270 - val_acc: 0.7500\n",
      "Epoch 13/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9215 - acc: 0.6121 - val_loss: 0.7331 - val_acc: 0.7500\n",
      "Epoch 14/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9187 - acc: 0.6121 - val_loss: 0.7356 - val_acc: 0.7500\n",
      "Epoch 15/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9203 - acc: 0.6121 - val_loss: 0.7406 - val_acc: 0.7500\n",
      "Epoch 16/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9199 - acc: 0.6121 - val_loss: 0.7416 - val_acc: 0.7500\n",
      "Epoch 17/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9163 - acc: 0.6121 - val_loss: 0.7416 - val_acc: 0.7500\n",
      "Epoch 18/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9167 - acc: 0.6121 - val_loss: 0.7311 - val_acc: 0.7500\n",
      "Epoch 19/20\n",
      "116/116 [==============================] - 1s 4ms/step - loss: 0.9229 - acc: 0.6121 - val_loss: 0.7232 - val_acc: 0.7500\n",
      "Epoch 20/20\n",
      "116/116 [==============================] - 1s 4ms/step - loss: 0.9101 - acc: 0.6121 - val_loss: 0.7221 - val_acc: 0.7500\n",
      "fold 2\n",
      "(116, 1, 53, 64, 46)\n",
      "Train on 116 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9138 - acc: 0.6293 - val_loss: 0.7212 - val_acc: 0.6500\n",
      "Epoch 2/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9117 - acc: 0.6293 - val_loss: 0.7212 - val_acc: 0.6500\n",
      "Epoch 3/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9116 - acc: 0.6293 - val_loss: 0.7233 - val_acc: 0.6500\n",
      "Epoch 4/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9101 - acc: 0.6293 - val_loss: 0.7277 - val_acc: 0.6500\n",
      "Epoch 5/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9142 - acc: 0.6293 - val_loss: 0.7309 - val_acc: 0.6500\n",
      "Epoch 6/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9103 - acc: 0.6293 - val_loss: 0.7309 - val_acc: 0.6500\n",
      "Epoch 7/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9089 - acc: 0.6293 - val_loss: 0.7316 - val_acc: 0.6500\n",
      "Epoch 8/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9081 - acc: 0.6293 - val_loss: 0.7322 - val_acc: 0.6500\n",
      "Epoch 9/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9038 - acc: 0.6293 - val_loss: 0.7288 - val_acc: 0.6500\n",
      "Epoch 10/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.9035 - acc: 0.6293 - val_loss: 0.7267 - val_acc: 0.6500\n",
      "Epoch 11/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9035 - acc: 0.6293 - val_loss: 0.7234 - val_acc: 0.6500\n",
      "Epoch 12/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9059 - acc: 0.6293 - val_loss: 0.7252 - val_acc: 0.6500\n",
      "Epoch 13/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9019 - acc: 0.6293 - val_loss: 0.7239 - val_acc: 0.6500\n",
      "Epoch 14/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9022 - acc: 0.6293 - val_loss: 0.7251 - val_acc: 0.6500\n",
      "Epoch 15/20\n",
      "116/116 [==============================] - 1s 4ms/step - loss: 0.9034 - acc: 0.6293 - val_loss: 0.7264 - val_acc: 0.6500\n",
      "Epoch 16/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9029 - acc: 0.6293 - val_loss: 0.7281 - val_acc: 0.6500\n",
      "Epoch 17/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.9012 - acc: 0.6293 - val_loss: 0.7292 - val_acc: 0.6500\n",
      "Epoch 18/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.8932 - acc: 0.6293 - val_loss: 0.7319 - val_acc: 0.6500\n",
      "Epoch 19/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.8974 - acc: 0.6293 - val_loss: 0.7290 - val_acc: 0.6500\n",
      "Epoch 20/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.8975 - acc: 0.6293 - val_loss: 0.7287 - val_acc: 0.6500\n",
      "fold 3\n",
      "(116, 1, 53, 64, 46)\n",
      "Train on 116 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.8088 - acc: 0.6724 - val_loss: 1.2695 - val_acc: 0.4000\n",
      "Epoch 2/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.8010 - acc: 0.6724 - val_loss: 1.2993 - val_acc: 0.4000\n",
      "Epoch 3/20\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.8042 - acc: 0.6724 - val_loss: 1.3087 - val_acc: 0.4000\n",
      "Epoch 4/20\n",
      "116/116 [==============================] - 1s 4ms/step - loss: 0.7964 - acc: 0.6724 - val_loss: 1.3255 - val_acc: 0.4000\n",
      "Epoch 5/20\n",
      "116/116 [==============================] - 1s 4ms/step - loss: 0.8003 - acc: 0.6724 - val_loss: 1.3320 - val_acc: 0.4000\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7990 - acc: 0.6724 - val_loss: 1.3215 - val_acc: 0.4000\n",
      "Epoch 7/20\n",
      "116/116 [==============================] - 1s 4ms/step - loss: 0.8017 - acc: 0.6724 - val_loss: 1.3054 - val_acc: 0.4000\n",
      "Epoch 8/20\n",
      "116/116 [==============================] - 1s 4ms/step - loss: 0.8003 - acc: 0.6724 - val_loss: 1.3102 - val_acc: 0.4000\n",
      "Epoch 9/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7943 - acc: 0.6724 - val_loss: 1.3063 - val_acc: 0.4000\n",
      "Epoch 10/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7934 - acc: 0.6724 - val_loss: 1.3116 - val_acc: 0.4000\n",
      "Epoch 11/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7942 - acc: 0.6724 - val_loss: 1.3245 - val_acc: 0.4000\n",
      "Epoch 12/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7930 - acc: 0.6724 - val_loss: 1.3373 - val_acc: 0.4000\n",
      "Epoch 13/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7944 - acc: 0.6724 - val_loss: 1.3283 - val_acc: 0.4000\n",
      "Epoch 14/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7894 - acc: 0.6724 - val_loss: 1.3228 - val_acc: 0.4000\n",
      "Epoch 15/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7974 - acc: 0.6724 - val_loss: 1.3193 - val_acc: 0.4000\n",
      "Epoch 16/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7893 - acc: 0.6724 - val_loss: 1.3145 - val_acc: 0.4000\n",
      "Epoch 17/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7970 - acc: 0.6724 - val_loss: 1.3270 - val_acc: 0.4000\n",
      "Epoch 18/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7957 - acc: 0.6724 - val_loss: 1.3207 - val_acc: 0.4000\n",
      "Epoch 19/20\n",
      "116/116 [==============================] - 1s 4ms/step - loss: 0.7837 - acc: 0.6724 - val_loss: 1.3255 - val_acc: 0.4000\n",
      "Epoch 20/20\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.7900 - acc: 0.6724 - val_loss: 1.3333 - val_acc: 0.4000\n",
      "fold 4\n",
      "(117, 1, 53, 64, 46)\n",
      "Train on 117 samples, validate on 19 samples\n",
      "Epoch 1/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.9145 - acc: 0.6068 - val_loss: 0.5665 - val_acc: 0.7895\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.9105 - acc: 0.6068 - val_loss: 0.5800 - val_acc: 0.7895\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.9039 - acc: 0.6068 - val_loss: 0.6000 - val_acc: 0.7895\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.9015 - acc: 0.6068 - val_loss: 0.6185 - val_acc: 0.7895\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.9022 - acc: 0.6068 - val_loss: 0.6181 - val_acc: 0.7895\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.9023 - acc: 0.6068 - val_loss: 0.6186 - val_acc: 0.7895\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8962 - acc: 0.6068 - val_loss: 0.6133 - val_acc: 0.7895\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.9036 - acc: 0.6068 - val_loss: 0.6057 - val_acc: 0.7895\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8928 - acc: 0.6068 - val_loss: 0.6033 - val_acc: 0.7895\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8979 - acc: 0.6068 - val_loss: 0.6011 - val_acc: 0.7895\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8894 - acc: 0.6068 - val_loss: 0.5973 - val_acc: 0.7895\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8961 - acc: 0.6068 - val_loss: 0.5934 - val_acc: 0.7895\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8920 - acc: 0.6068 - val_loss: 0.5917 - val_acc: 0.7895\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8859 - acc: 0.6068 - val_loss: 0.5962 - val_acc: 0.7895\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8938 - acc: 0.6068 - val_loss: 0.6002 - val_acc: 0.7895\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8974 - acc: 0.6068 - val_loss: 0.6051 - val_acc: 0.7895\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8947 - acc: 0.6068 - val_loss: 0.6066 - val_acc: 0.7895\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8938 - acc: 0.6068 - val_loss: 0.6030 - val_acc: 0.7895\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8872 - acc: 0.6068 - val_loss: 0.5971 - val_acc: 0.7895\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8898 - acc: 0.6068 - val_loss: 0.5933 - val_acc: 0.7895\n",
      "fold 5\n",
      "(117, 1, 53, 64, 46)\n",
      "Train on 117 samples, validate on 19 samples\n",
      "Epoch 1/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8354 - acc: 0.6410 - val_loss: 0.9369 - val_acc: 0.5789\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8412 - acc: 0.6410 - val_loss: 0.9386 - val_acc: 0.5789\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8310 - acc: 0.6410 - val_loss: 0.9427 - val_acc: 0.5789\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8333 - acc: 0.6410 - val_loss: 0.9450 - val_acc: 0.5789\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8348 - acc: 0.6410 - val_loss: 0.9478 - val_acc: 0.5789\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8303 - acc: 0.6410 - val_loss: 0.9487 - val_acc: 0.5789\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8338 - acc: 0.6410 - val_loss: 0.9471 - val_acc: 0.5789\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8359 - acc: 0.6410 - val_loss: 0.9454 - val_acc: 0.5789\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8279 - acc: 0.6410 - val_loss: 0.9465 - val_acc: 0.5789\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8323 - acc: 0.6410 - val_loss: 0.9486 - val_acc: 0.5789\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8264 - acc: 0.6410 - val_loss: 0.9486 - val_acc: 0.5789\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8313 - acc: 0.6410 - val_loss: 0.9486 - val_acc: 0.5789\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8289 - acc: 0.6410 - val_loss: 0.9496 - val_acc: 0.5789\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8214 - acc: 0.6410 - val_loss: 0.9493 - val_acc: 0.5789\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8262 - acc: 0.6410 - val_loss: 0.9508 - val_acc: 0.5789\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8181 - acc: 0.6410 - val_loss: 0.9524 - val_acc: 0.5789\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8286 - acc: 0.6410 - val_loss: 0.9543 - val_acc: 0.5789\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8209 - acc: 0.6410 - val_loss: 0.9520 - val_acc: 0.5789\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8332 - acc: 0.6410 - val_loss: 0.9513 - val_acc: 0.5789\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8301 - acc: 0.6410 - val_loss: 0.9502 - val_acc: 0.5789\n",
      "fold 6\n",
      "(117, 1, 53, 64, 46)\n",
      "Train on 117 samples, validate on 19 samples\n",
      "Epoch 1/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8370 - acc: 0.6325 - val_loss: 0.8729 - val_acc: 0.6316\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8328 - acc: 0.6325 - val_loss: 0.8726 - val_acc: 0.6316\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8395 - acc: 0.6325 - val_loss: 0.8727 - val_acc: 0.6316\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8295 - acc: 0.6325 - val_loss: 0.8731 - val_acc: 0.6316\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8351 - acc: 0.6325 - val_loss: 0.8735 - val_acc: 0.6316\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8289 - acc: 0.6325 - val_loss: 0.8730 - val_acc: 0.6316\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8325 - acc: 0.6325 - val_loss: 0.8736 - val_acc: 0.6316\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8236 - acc: 0.6325 - val_loss: 0.8735 - val_acc: 0.6316\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8279 - acc: 0.6325 - val_loss: 0.8743 - val_acc: 0.6316\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8298 - acc: 0.6325 - val_loss: 0.8740 - val_acc: 0.6316\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8234 - acc: 0.6325 - val_loss: 0.8743 - val_acc: 0.6316\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8280 - acc: 0.6325 - val_loss: 0.8737 - val_acc: 0.6316\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8305 - acc: 0.6325 - val_loss: 0.8738 - val_acc: 0.6316\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8376 - acc: 0.6325 - val_loss: 0.8739 - val_acc: 0.6316\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8306 - acc: 0.6325 - val_loss: 0.8742 - val_acc: 0.6316\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8260 - acc: 0.6325 - val_loss: 0.8743 - val_acc: 0.6316\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8392 - acc: 0.6325 - val_loss: 0.8747 - val_acc: 0.6316\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8351 - acc: 0.6325 - val_loss: 0.8755 - val_acc: 0.6316\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8251 - acc: 0.6325 - val_loss: 0.8759 - val_acc: 0.6316\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8332 - acc: 0.6325 - val_loss: 0.8760 - val_acc: 0.6316\n",
      "fold 7\n",
      "(117, 1, 53, 64, 46)\n",
      "Train on 117 samples, validate on 19 samples\n",
      "Epoch 1/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8316 - acc: 0.6325 - val_loss: 0.8840 - val_acc: 0.6316\n",
      "Epoch 2/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8314 - acc: 0.6325 - val_loss: 0.8836 - val_acc: 0.6316\n",
      "Epoch 3/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8293 - acc: 0.6325 - val_loss: 0.8829 - val_acc: 0.6316\n",
      "Epoch 4/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8191 - acc: 0.6325 - val_loss: 0.8827 - val_acc: 0.6316\n",
      "Epoch 5/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8122 - acc: 0.6325 - val_loss: 0.8825 - val_acc: 0.6316\n",
      "Epoch 6/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8184 - acc: 0.6325 - val_loss: 0.8827 - val_acc: 0.6316\n",
      "Epoch 7/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8185 - acc: 0.6325 - val_loss: 0.8833 - val_acc: 0.6316\n",
      "Epoch 8/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8198 - acc: 0.6325 - val_loss: 0.8831 - val_acc: 0.6316\n",
      "Epoch 9/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8380 - acc: 0.6325 - val_loss: 0.8829 - val_acc: 0.6316\n",
      "Epoch 10/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8200 - acc: 0.6325 - val_loss: 0.8831 - val_acc: 0.6316\n",
      "Epoch 11/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8061 - acc: 0.6325 - val_loss: 0.8831 - val_acc: 0.6842\n",
      "Epoch 12/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8212 - acc: 0.6325 - val_loss: 0.8832 - val_acc: 0.6842\n",
      "Epoch 13/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8251 - acc: 0.6325 - val_loss: 0.8837 - val_acc: 0.6842\n",
      "Epoch 14/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8104 - acc: 0.6325 - val_loss: 0.8838 - val_acc: 0.6842\n",
      "Epoch 15/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8178 - acc: 0.6325 - val_loss: 0.8842 - val_acc: 0.6842\n",
      "Epoch 16/20\n",
      "117/117 [==============================] - 0s 4ms/step - loss: 0.8111 - acc: 0.6325 - val_loss: 0.8841 - val_acc: 0.6842\n",
      "Epoch 17/20\n",
      "117/117 [==============================] - 1s 4ms/step - loss: 0.8188 - acc: 0.6325 - val_loss: 0.8836 - val_acc: 0.6842\n",
      "Epoch 18/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8057 - acc: 0.6325 - val_loss: 0.8838 - val_acc: 0.6842\n",
      "Epoch 19/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8304 - acc: 0.6325 - val_loss: 0.8839 - val_acc: 0.6842\n",
      "Epoch 20/20\n",
      "117/117 [==============================] - 1s 5ms/step - loss: 0.8186 - acc: 0.6325 - val_loss: 0.8838 - val_acc: 0.6842\n"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "Kfold = KFold(n_splits=7)\n",
    "Kfold.get_n_splits(x_train, y_train)  \n",
    "foldNum=0                                  #initializing fold = 0\n",
    "for train_index, val_index in Kfold.split(x_train, y_train):\n",
    "    foldNum+=1\n",
    "    print(\"fold\",foldNum)\n",
    "    X_train, X_val = x_train[train_index], x_train[val_index]\n",
    "    Y_train, Y_val = y_train[train_index], y_train[val_index]\n",
    "    print(X_train.shape)\n",
    "    #training the dataset\n",
    "    history1 = model.fit(X_train, Y_train, validation_data = (X_val,Y_val), epochs=20,batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "25/25 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9176188111305237, 0.6399999856948853]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('FMRI_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('FMRI_weights.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#previous model \n",
    "model = keras.models.Sequential() \n",
    "model.add(keras.layers.Conv2D(32, kernel_size= 3, \n",
    "                                              kernel_initializer='he_normal',padding='same',\n",
    "                                              input_shape=(53,64,46)))\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size= 3, kernel_initializer='he_normal', padding = 'same',\n",
    "                              activation = 'relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size= 3, kernel_initializer='he_normal',padding = 'same',\n",
    "                              activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size = 2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size= 3, kernel_initializer='he_normal', padding = 'same',\n",
    "                              activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(250, kernel_initializer='he_normal',activation = 'relu'))\n",
    "model.add(keras.layers.Dense(250, kernel_initializer='he_normal',activation = 'relu'))\n",
    "model.add(keras.layers.Dense(250, kernel_initializer='he_normal',activation = 'relu'))\n",
    "model.add(keras.layers.Dense(2,activation = 'softmax'))\n",
    "#model.add(Activation('linear'))\n",
    "    \n",
    "#model.add(keras.layers.Activation('softmax'))\n",
    "    \n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
